{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas numpy matplotlib plotly seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_read = ['rqc_off_stats.txt', 'rqc_on_stats.txt']\n",
    "\n",
    "def read_file(file_name):\n",
    "    with open(file_name) as f:\n",
    "        lines = f.readlines()\n",
    "        return lines\n",
    "\n",
    "def get_rocksdb_compact_write_bytes(lines) -> List[str]:\n",
    "    return [line for line in lines if 'rocksdb.compact.write.bytes' in line]\n",
    "\n",
    "def get_rocksdb_flush_write_bytes(lines) -> List[str]:\n",
    "    return [line for line in lines if 'rocksdb.flush.write.bytes' in line]\n",
    "\n",
    "def get_rocksdb_total_db_bytes(lines) -> List[str]:\n",
    "    return [line for line in lines if 'Estimated live data size: ' in line]\n",
    "\n",
    "def transform_bytes(lines) -> List[int]:\n",
    "    return [int(line.strip(' \\n').split(' : ')[1]) for line in lines]\n",
    "\n",
    "def get_total_bytes(compact_bytes, flush_bytes) -> List[int]:\n",
    "    return [compact_bytes[i] + flush_bytes[i] for i in range(len(compact_bytes))]\n",
    "\n",
    "def get_levels_stats(lines) -> List[Tuple[str, int, int]]:\n",
    "    return [line.strip(' \\t') for line in lines if 'Level-' in line or 'Total:' in line]\n",
    "\n",
    "def get_total_files_and_entries(lines) -> Tuple[int, int]:\n",
    "    total_files = 0\n",
    "    total_entries = 0\n",
    "\n",
    "    for line in lines:\n",
    "        if 'Total' in line:\n",
    "            values = line.strip(' \\t').split() \n",
    "            total_files = int(values[1])\n",
    "            total_entries = int(values[2])\n",
    "            break\n",
    "    \n",
    "    return total_files, total_entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Space Amplification for different values of lower bound and upper bound ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = os.listdir('./')\n",
    "\n",
    "results = list()\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    if not os.path.isdir(experiment):\n",
    "        continue\n",
    "\n",
    "    i, inserts, u, updates, s, range_queries, y, selectivity, t, size_ratio, *others = experiment.split(' ')\n",
    "    \n",
    "    if len(others) == 2:\n",
    "        rq, rqc = others\n",
    "        lb, lb_value = None, 0\n",
    "        ub, ub_value = None, 0\n",
    "    else:\n",
    "       lb, lb_value, ub, ub_value, rq, rqc = others\n",
    "\n",
    "    if rqc =='0':\n",
    "        approach = 'vanilla'\n",
    "        lines = read_file(f'{experiment}/rqc_off_stats.txt')\n",
    "    else:\n",
    "        approach = 'rqdc'\n",
    "        lines = read_file(f'{experiment}/rqc_on_stats.txt')\n",
    "\n",
    "    # files and entries\n",
    "    sst_file_size_ = read_file(f'{experiment}/sst_file_size_and_count.txt')\n",
    "    total_files, totat_db_size = sst_file_size_[0].split('\\t')\n",
    "    iperf_total_files, iperf_total_entries = get_total_files_and_entries(lines)\n",
    "\n",
    "    iperf_total_size = get_rocksdb_total_db_bytes(lines)[0].strip(' \\n').split(' : ')[1]\n",
    "\n",
    "    # write bytes\n",
    "    compact_write_bytes = get_rocksdb_compact_write_bytes(lines)\n",
    "    flush_write_bytes = get_rocksdb_flush_write_bytes(lines)\n",
    "\n",
    "    compact_bytes = transform_bytes(compact_write_bytes)\n",
    "    flush_bytes = transform_bytes(flush_write_bytes)\n",
    "\n",
    "\n",
    "    results.append({\n",
    "        'inserts': inserts,\n",
    "        'updates': updates,\n",
    "        'range_queries': range_queries,\n",
    "        'size_ratio': size_ratio,\n",
    "        'rqc': rqc,\n",
    "        'lb': lb_value,\n",
    "        'ub': ub_value,\n",
    "        'approach': approach,\n",
    "        'iperf_total_size': iperf_total_size,\n",
    "        'iperf_total_files': iperf_total_files,\n",
    "        'total_files': int(total_files.strip()),\n",
    "        'total_db_size': int(totat_db_size.strip()),\n",
    "        'compact_write_bytes': compact_bytes[-1],\n",
    "        'flush_write_bytes': flush_bytes[-1],\n",
    "        'total_write_bytes': get_total_bytes(compact_bytes, flush_bytes)[-1],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "# Extract the 'total_files' column\n",
    "rqdc_total_files = df[df['approach'] == 'rqdc']\n",
    "total_files = rqdc_total_files['total_files']\n",
    "\n",
    "# Calculate percentiles\n",
    "percentiles = [90, 95, 99]\n",
    "percentile_values = np.percentile(total_files, percentiles)\n",
    "mean_value = np.mean(total_files)\n",
    "std_dev_value = np.std(total_files)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'total_files']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='total_files')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "\n",
    "# Add annotations for percentiles above the heatmap\n",
    "plt.text(0.5, -0.15, \n",
    "         f'RQDC Mean: {mean_value:.2f}   |   RQDC Std Dev: {std_dev_value:.2f}   |   \\nRQDC 90th Percentile: {percentile_values[0]:.2f}   |   RQDC 95th Percentile: {percentile_values[1]:.2f}   |   RQDC 99th Percentile: {percentile_values[2]:.2f}', \n",
    "         horizontalalignment='center', verticalalignment='center', fontsize=12, color='blue', weight='bold', transform=plt.gca().transAxes)\n",
    "\n",
    "plt.title('Actual Number of (SST) Files in System')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'total_db_size']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='total_db_size')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Actual DB Size in System')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'iperf_total_files']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='iperf_total_files')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Total Files Given by RocksDB stats at the end of Experiment')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'iperf_total_size']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='iperf_total_size')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Total DB Size Given By RocksDB Stats at the end of Experiment')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write Amplification for different values of Lower Bound and Upper Bound ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'total_write_bytes']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='total_write_bytes')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Total Bytes Written for different lower bound and upper bound')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'compact_write_bytes']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='compact_write_bytes')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Total Bytes Written during compactions for different lower bound and upper bound ratio')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'flush_write_bytes']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='flush_write_bytes')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Total Bytes Written while flushes + RQDC for different lower bound and upper bound ratio')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workload Latency for different values of lower bound and upper bound ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = os.listdir('./')\n",
    "\n",
    "results = list()\n",
    "\n",
    "for experiment in experiments:\n",
    "\n",
    "    if not os.path.isdir(experiment):\n",
    "        continue\n",
    "\n",
    "    i, inserts, u, updates, s, range_queries, y, selectivity, t, size_ratio, *others = experiment.split(' ')\n",
    "    \n",
    "    if len(others) == 2:\n",
    "        rq, rqc = others\n",
    "        lb, lb_value = None, 0\n",
    "        ub, ub_value = None, 0\n",
    "    else:\n",
    "        lb, lb_value, ub, ub_value, rq, rqc = others\n",
    "\n",
    "    if rqc =='0':\n",
    "        approach = 'vanilla'\n",
    "        compacted_vs_skipped = pd.read_csv(f'{experiment}/rqc_off_compacted_vs_skipped.csv')\n",
    "        stats = pd.read_csv(f'{experiment}/rqc_off_stats.csv', names=['QueryNo', 'Type', 'Key', 'TotalTimeTaken', 'SuccessFailDecisionForRQDC'])\n",
    "    else:\n",
    "        approach = 'rqdc'\n",
    "        compacted_vs_skipped = pd.read_csv(f'{experiment}/rqc_on_compacted_vs_skipped.csv')\n",
    "        stats = pd.read_csv(f'{experiment}/rqc_on_stats.csv', names=['QueryNo', 'Type', 'Key', 'TotalTimeTaken', 'SuccessFailDecisionForRQDC'])\n",
    "\n",
    "    total_workload_time = stats[stats['Type'] == ' Total']\n",
    "    total_time_taken = total_workload_time.reset_index()['TotalTimeTaken']\n",
    "\n",
    "    # take the average of total time taken for range queries\n",
    "    total_range_time = stats[stats['Type'] == ' Range']\n",
    "    total_range_time_taken = total_range_time.reset_index()['TotalTimeTaken']\n",
    "    avg_range_time_taken = np.mean(total_range_time_taken)\n",
    "\n",
    "    # count the number of time we see on average RQDC is successful\n",
    "    total_range_time_2 = stats[stats['Type'] == ' Range']\n",
    "    avg_times_success = total_range_time_2.reset_index()['SuccessFailDecisionForRQDC']\n",
    "    avg_times_success = np.sum(avg_times_success)\n",
    "\n",
    "    results.append({\n",
    "        'inserts': inserts,\n",
    "        'updates': updates,\n",
    "        'range_queries': range_queries,\n",
    "        'size_ratio': size_ratio,\n",
    "        'rqc': rqc,\n",
    "        'lb': lb_value,\n",
    "        'ub': ub_value,\n",
    "        'approach': approach,\n",
    "        'total_time_taken': total_time_taken[0],\n",
    "        'average_time_taken_by_range_queries': avg_range_time_taken,\n",
    "        'avg_time_success': avg_times_success,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'total_time_taken']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='total_time_taken')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Total time taken by each workload for different lower bound and upper bound ratio (in seconds)')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Time taken by range queries for different values of lower bound and upper bound ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'average_time_taken_by_range_queries']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='average_time_taken_by_range_queries')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Average total time taken by Range queries for different lower bound and upper bound ratio')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of Times the RQDC is Success for different values of lower bound and upper bound ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(results)\n",
    "\n",
    "df.sort_values(by=['lb', 'ub'], inplace=True)\n",
    "\n",
    "number_of_files_df = df[['lb', 'ub', 'avg_time_success']]\n",
    "\n",
    "number_of_files_df.head()\n",
    "\n",
    "heatmap_data = df.pivot_table(index='ub', columns='lb', values='avg_time_success')\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "plt.figure(figsize=(16, 8))\n",
    "sns.heatmap(heatmap_data, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", linewidths=.5)\n",
    "plt.title('Number of time RQDC was success (out of 100) for different lower bound and upper bound ratio')\n",
    "plt.xlabel('lower bound')\n",
    "plt.ylabel('upper bound')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
